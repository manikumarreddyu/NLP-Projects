 Hello everyone, welcome to this second video on this three video series where we are discussing about how to generate sentence embeddings from sentence transformers. Okay, so in this previous video, we generated our own embeddings from sentence transformers, right? Embeddings were generated, but the main issue was it was having 384 dimensions. So it would be even 1024 as I said, okay, but if you are going to use this embeddings for ML models, okay, ML models, they might not be able to fit their mathematical equation for this. Obviously for deep learning neural network, it will work well, but if you are going for machine learning models, it's not going to work well as per theory because they are not going to learn about nonlinear approaches. They only know to fit their mathematical equation, right? So for that, what we can do is we can do dimensionality reduction. So what is this dimensionality reduction? It will try to reduce a dimension like it will be projecting your dimension into a limited number of dimensions. If there is a value of around 384, 384 values, and if we want to project this into 10 dimensions, it will be projecting. So that is called dimensionality reduction. Okay, so there are a lot of approaches for doing this dimensionality reductions like PCA, LDA, TSNE, but we are going to use one of the best approaches which is available now, which is UMAP. Okay UMAP is called as uniform, uniform, manifold, approximation, and projection. Okay for what? for dimensionality reduction. Okay, so that is UMAP. What it is, what is this unique, UMAP is that it is a dimensionality reduction technique as well, okay, which is almost similar to TSNE, but this is better than TSNE. Why? This is not limited to two or three dimensions. Okay, two, three dimension, it is not limited to that. You can generate how many dimensions you want. to reduce it to 10, you can do that. And the data global structure. What do I mean by this data global structure? Let's see that. Okay, this should be maintained. For example, if I'm having a data which is distributed like this, it's not going to be like this, which is obviously a nonlinear data. So what if you're considering the data is distributed like this, okay, and if you're going to use TSNE, it might go like this or else even like this. Okay, but if you are going to for UMAP, it would be almost like this. Okay, sorry, not like this. We are just followed, which is almost not disturbing the distribution of the data. The global structure of the data is same and the distance between the clusters is easily calculated. Because these dimensionality reduction techniques are based on clustering. Okay, it will try to cluster the data from there only it will try to reduce the data. Okay, dimension, there is a data dimension. So here the distance calculation between the dimensions of data is very easier. Okay, so this is one of the state of thought approaches to reduce the dimensions of embeddings. Okay, so let's see what it is then. So first to use this UMAP, you need to install a library called UMAP-learn. Okay, so let's do this, install UMAP-learn. And then from UMAP and calling the function called UMAP and obviously pandas is there, minmap scaler is there. Okay, send this transformer if it is there, obviously like, you know, to generate embeddings we need it right, we saw it in the previous video and TQDM and MPI as usual as in the previous video itself. So I'm having the same embedding generator function and input gen function, which I think I don't need to explain if you want to know what is this, go and see the previous video of the series itself. Okay, so next we are having dimensionality reduction. Okay, so what is this function is that first I'm taking, sorry, let me go for white. Okay, so yeah, I'm taking two inputs. First is the embedding array. What do you mean by embedding array is that embedding dimension array, okay, we converted it right, like from an array we converted into an array of elements, an array of array we converted into an array of elements right, this A from this input gen. So that only we are going to use it here and then the label obviously, the label is nothing but the text, the label column of data frame. So what we are doing first is we are doing min max scalar, because there is a negative values and positive values, it will be good if we are having it from zero to one. So we are doing min max scalar. I think you all know the formula of it, x minus x min by x max minus x min. Okay, this would be the formula of min max scalar. It is also called as normalization. The process is normalization of data. So for here, I'm creating a mapper, mapper is just a variable, but it is kind of a self explanatory variable, because we are creating an object which will try to map the given dimension to the number of dimension which we need. For example, here I have given n components equal to two, which is the number of components I need. But you might need how many dimensions you want, you can just give it 10, 15, 20, how much you want, you can just give it away. And then we are having metric equal to cosine. So to calculate the similarity, we are using cosine similarity. And then I am fitting this x scale. So by this, I'm just getting the mapper embedding, and then I'm creating a column out of it where there will be x and y. Okay, so usually we go, you should go for x and y, because we should go for two to three dimension access, and then we'll be plotting it if we are going for three dimension in So that's why we just give an x and y, but if you want, you can give it as embedding one embedding two, how much you want, you can do it. So again, we are going to do the same call. But here, I'm just going to take this embedding output from input, gene input, and then I'm putting it into dimensional deduction. So let's see what happens here. So it is going to take that same 30 seconds, not more than that. Okay, so now what we're doing is the value which was there in 384 dimension, we are trying to map it, we are trying to compress the dimension. It is not just projecting the dimension, it is just compressing the dimension and then it is projecting it into two dimensions. Okay, that's what this dimensionality reduction does. We're having two dimensions, right? So if you want to have a clear look, here it is. So by this, it will be kind of easier for us to train a model, right? But two columns might not be enough. So that's why I said we can change it to how many columns you want, five, six, 10, how much you want, we can use it. Even if you want 384, just don't do this embedding reduction at all. Just go for the 384 embedding itself. But there was a main purpose to reduce this because in the next video, I'll be showing about how to visualize this embeddings. We'll be using a specific type of visualization called hex bin. So hex bin will be showing how much it is distributed for specific classes. We'll be seeing that because it will be useful for us to gain some insights. So by using UMAP, you can easily reduce your dimensions. But this UMAP is not just for this embeddings. You can also do it for your normal ML data preprocessing also. So thank you guys. I'll see you all in the next video. Until then, bye bye.